{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import #style_guide\n",
    "from __future__ import division        #style_guide\n",
    "from __future__ import print_function  #style_guide\n",
    "\n",
    "import os  #file access\n",
    "import csv #output results\n",
    "import time  #timing epochs\n",
    "from datetime import timedelta #timing epochs\n",
    "\n",
    "import math #might not need\n",
    "import numpy as np #math/arrays\n",
    "import tensorflow as tf #NN\n",
    "import tflearn #might not need - NN if I do\n",
    "import matplotlib.pyplot as plt #for plotting data\n",
    "\n",
    "import PIL\n",
    "from PIL import Image #image processing\n",
    "from zipfile import ZipFile #File processing\n",
    "from tqdm import tqdm_notebook as tqdm #Progress Bar\n",
    "\n",
    "from sklearn.utils import shuffle  #Shuffling training data\n",
    "from sklearn.preprocessing import LabelBinarizer  #creating label set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "## image/general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width = 32\n",
    "img_height = 18\n",
    "img_size_flat = img_height * img_width\n",
    "\n",
    "img_shape = (img_width, img_height)\n",
    "arr_shape = (img_height, img_width)\n",
    "\n",
    "#RGB\n",
    "num_channels = 3\n",
    "\n",
    "#Number of classes. In order(minus image):\n",
    "csv_header = ['image', 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "num_classes = 8\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_pix(pixels):\n",
    "    norm = np.array(pixels.shape)\n",
    "    a=.1\n",
    "    b=.9\n",
    "    x_max = 255\n",
    "    x_min = 0\n",
    "    \n",
    "    norm = a + ((pixels - x_min)*(b-a)/(x_max-x_min))\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    global img_size\n",
    "    features = []\n",
    "    labels   = []\n",
    "    current_label = \"\"\n",
    "    \n",
    "    with ZipFile(file) as zf:\n",
    "        #Progress Bar\n",
    "        filenames_pbar = tqdm(zf.namelist(), unit='files')\n",
    "        #Features and labels\n",
    "        for i, filename in enumerate(filenames_pbar):\n",
    "            if not filename.endswith('/') and filename.endswith('.jpg'):\n",
    "                with zf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file).resize(img_shape,resample = PIL.Image.LANCZOS)\n",
    "                    image_arr = normalize_pix(np.array(image, dtype=np.float32))\n",
    "                    if filename[6:9] != current_label:\n",
    "                        current_label = filename[6:9] if \"/\" in filename[6:10] else filename[6:11]\n",
    "\n",
    "                    features.append(image_arr)\n",
    "                    labels.append(current_label)\n",
    "                       \n",
    "    return np.array(features),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prep(file):\n",
    "    features, labels = uncompress_features_labels(file)\n",
    "    \n",
    "    #binarize for use with class_vector\n",
    "    labels = binarize_labels(labels)\n",
    "    \n",
    "    #shuffle data\n",
    "    features,labels = shuffle(features, labels)\n",
    "\n",
    "    #break into train and validation data\n",
    "    #3527 training_samples\n",
    "    train_features  = features[:3527]\n",
    "    train_labels    = labels[:3527]\n",
    "\n",
    "    #250 val_samples\n",
    "    valid_features  = features[3527:]\n",
    "    valid_labels    = labels[3527:]\n",
    "    \n",
    "    return (train_features,train_labels),(valid_features,valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create matrix of weights for given shape from Gaussian dist. with stddev of .05,\n",
    "# truncated so anything over st. dev. 2 is re-sampled\n",
    "def new_weights(shape,name='W'):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates new biases for a NN layer\n",
    "def new_biases(length,name='B'):\n",
    "    return tf.Variable(tf.zeros(shape=[length]), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes input, weight matrix, and name, and returns a 2d convolution tensor\n",
    "# with stride 1 and padding SAME, used in new_inception_layer function\n",
    "def conv2d_s1(x,W,name='conv_s1'):\n",
    "    return tf.nn.conv2d(input=x,\n",
    "                        filter=W,\n",
    "                        strides=[1,1,1,1],\n",
    "                        padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns a max_pooled version of input x with size 3x3 and stride 1\n",
    "def max_pool_3x3_s1(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,3,3,1], strides=[1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates an inception module, with 1x1 convolution that feeds seperately into the output, a 3x3 convolution filter,\n",
    "# and a 5x5 convolution filter. In addition, a 3x3 max_pool is of the input is fed into another 1x1 filter, and all\n",
    "# 4 are concatenated, to create the final output\n",
    "\n",
    "def new_inception_layer(x, depth1x1, num_filters,\n",
    "                        num_channels,name='Incep'):\n",
    "    with tf.name_scope(name):\n",
    "        # connected to input\n",
    "        # direct 1x1 convolution\n",
    "        W_conv_1x1_1_name = 'W_1x1_1_{}x{}x{}x{}'.format(1,1,num_channels,num_filters)\n",
    "        W_conv_1x1_1  = new_weights([1,1,num_channels,num_filters],name=W_conv_1x1_1_name) #1x1x3x32\n",
    "        b_conv_1x1_1 = new_biases(num_filters,name='B_{}'.format(num_filters))\n",
    "        tf.summary.histogram(W_conv_1x1_1_name,W_conv_1x1_1)\n",
    "        tf.summary.histogram('B_{}'.format(num_filters),b_conv_1x1_1)\n",
    "\n",
    "        # connected to input\n",
    "        # dim_reduc before 3x3 convolution\n",
    "        W_conv_1x1_2_name = 'W_1x1_2_{}x{}x{}x{}'.format(1,1,num_channels,depth1x1)\n",
    "        W_conv_1x1_2 = new_weights([1,1,num_channels,depth1x1],name=W_conv_1x1_2_name)\n",
    "        b_conv_1x1_2 = new_biases(depth1x1,name='B_{}'.format(depth1x1))\n",
    "        tf.summary.histogram(W_conv_1x1_2_name,W_conv_1x1_2)\n",
    "        tf.summary.histogram('B_{}'.format(depth1x1),b_conv_1x1_2)\n",
    "        \n",
    "        # connected to inputs\n",
    "        # dim reduc before5x5 convolution\n",
    "        W_conv_1x1_3_name = 'W_1x1_3_{}x{}x{}x{}'.format(1,1,num_channels,depth1x1)\n",
    "        W_conv_1x1_3 = new_weights([1,1,num_channels,depth1x1],name=W_conv_1x1_3_name)\n",
    "        b_conv_1x1_3 = new_biases(depth1x1,name='B_{}'.format(depth1x1))\n",
    "        tf.summary.histogram(W_conv_1x1_3_name,W_conv_1x1_3)\n",
    "        tf.summary.histogram('B_{}'.format(depth1x1),b_conv_1x1_3)\n",
    "\n",
    "        # connected to 1x1_2\n",
    "        W_conv_3x3_name = 'W_3x3_{}x{}x{}x{}'.format(3,3,depth1x1,num_filters)\n",
    "        W_conv_3x3 = new_weights([3,3,depth1x1,num_filters],name=W_conv_3x3_name)\n",
    "        b_conv_3x3  = new_biases(num_filters,name='B_{}'.format(num_filters))\n",
    "        tf.summary.histogram(W_conv_3x3_name,W_conv_3x3)\n",
    "        tf.summary.histogram('B_{}'.format(num_filters),b_conv_3x3)\n",
    "            \n",
    "        # connected to 1x1_3\n",
    "        W_conv_5x5_name = 'W_5x5_{}x{}x{}x{}'.format(5,5,depth1x1,num_filters)\n",
    "        W_conv_5x5 = new_weights([5,5,depth1x1,num_filters],name=W_conv_5x5_name)\n",
    "        b_conv_5x5  = new_biases(num_filters,name='B_{}'.format(num_filters))\n",
    "        tf.summary.histogram(W_conv_5x5_name,W_conv_5x5)\n",
    "        tf.summary.histogram('B_{}'.format(num_filters),b_conv_5x5)\n",
    "\n",
    "        # connected to max_pool\n",
    "        W_conv_1x1_4_name = 'W_1x1_4_{}x{}x{}x{}'.format(1,1,num_channels,depth1x1)\n",
    "        W_conv_1x1_4 = new_weights([1,1,num_channels,depth1x1],name=W_conv_1x1_4_name)\n",
    "        b_conv_1x1_4 = new_biases(depth1x1,name='B_{}'.format(depth1x1))\n",
    "        tf.summary.histogram(W_conv_1x1_4_name,W_conv_1x1_4)\n",
    "        tf.summary.histogram('B_{}'.format(depth1x1),b_conv_1x1_4)\n",
    "\n",
    "        conv_1x1_1 = conv2d_s1(x,W_conv_1x1_1)+b_conv_1x1_1\n",
    "        conv_1x1_2 = tf.nn.relu(conv2d_s1(x,W_conv_1x1_2)+b_conv_1x1_2)\n",
    "        conv_1x1_3 = tf.nn.relu(conv2d_s1(x,W_conv_1x1_3)+b_conv_1x1_3)\n",
    "        conv_3x3 = conv2d_s1(conv_1x1_2,W_conv_3x3) + b_conv_3x3\n",
    "        conv_5x5 = conv2d_s1(conv_1x1_3,W_conv_5x5) + b_conv_5x5\n",
    "        max_pool = max_pool_3x3_s1(x)\n",
    "        conv_1x1_4 = conv2d_s1(max_pool,W_conv_1x1_4) + b_conv_1x1_4\n",
    "\n",
    "        inception = tf.nn.relu(tf.concat(values=[conv_1x1_1,conv_3x3,conv_5x5,conv_1x1_4],axis=3))\n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Defines a 2d convolution tensor layer\n",
    "#Has ReLU and pooling built in\n",
    "def new_conv_layer(input,\n",
    "                   num_input_channels,\n",
    "                   filter_size,\n",
    "                   num_filters,\n",
    "                   filter_stride=1,\n",
    "                   use_pooling=2,\n",
    "                   name='Conv'):\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        #define the shape for weight shape\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "        #define matrix of weights\n",
    "        weights_name = 'W_{}x{}x{}x{}'.format(filter_size, filter_size, num_input_channels, num_filters)\n",
    "        weights = new_weights(shape=shape,name=weights_name)\n",
    "        tf.summary.histogram(weights_name,weights)\n",
    "\n",
    "        #define vector of biases\n",
    "        biases  = new_biases(length=num_filters,name='B_{}'.format(num_filters))\n",
    "        tf.summary.histogram('B_{}'.format(num_filters),biases)\n",
    "        #define actual tensorflow cn layers\n",
    "        layer = tf.nn.conv2d(input=input,\n",
    "                             filter=weights,\n",
    "                             strides=[1, filter_stride, filter_stride, 1],\n",
    "                             padding='SAME')\n",
    "\n",
    "        #add biases\n",
    "        layer += biases\n",
    "\n",
    "        #if use_pooling > 1, does a max pooling with filter size and stride of size use_pooling\n",
    "        if use_pooling > 1:\n",
    "            layer = tf.nn.max_pool(value=layer,\n",
    "                                   ksize=[1, use_pooling, use_pooling, 1],\n",
    "                                   strides=[1, use_pooling, use_pooling, 1],\n",
    "                                   padding='SAME')\n",
    "\n",
    "        #if change to ave_pooling, switch order of ReLU and pooling!\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Flattens the input layer for use with FC NN layer\n",
    "def flatten_layer(layer):\n",
    "    with tf.name_scope('flatten'):\n",
    "        layer_shape = layer.get_shape()\n",
    "\n",
    "        #img_height*img_width*num_channels\n",
    "        num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "        #flatten layer to num_images, num_features\n",
    "        layer_flat = tf.reshape(layer, [-1,num_features])\n",
    "    \n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates a new fully-connected NN layer, used after Convolution layers\n",
    "def new_fc_layer(input,\n",
    "                 num_inputs,\n",
    "                 num_outputs,\n",
    "                 use_relu=True,\n",
    "                 name='FC'):\n",
    "    with tf.name_scope(name):\n",
    "        #define weights and biases\n",
    "        weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "        biases  = new_biases(length=num_outputs)\n",
    "\n",
    "        #linear combination, tensor object\n",
    "        layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "        #if using non-linearity activation\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split sets into batches\n",
    "def fetch_batches(batch_size,features,labels):\n",
    "    feature_batches = []\n",
    "    label_batches   = []\n",
    "    \n",
    "    for start_i in range(0,len(features), batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        \n",
    "        f_batch = features[start_i:end_i]\n",
    "        l_batch = labels[start_i:end_i]\n",
    "        \n",
    "        feature_batches.append(f_batch)\n",
    "        label_batches.append(l_batch)\n",
    "        \n",
    "    return feature_batches, label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tag input labels and create a list of length num_classes, with all 0's except the correct class, which is 1\n",
    "def binarize_labels(label_set):\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(label_set)\n",
    "    \n",
    "    label_set = encoder.transform(label_set)\n",
    "    label_set = label_set.astype(np.float32)\n",
    "    return label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a tensorflow graph\n",
    "def save_model(file_name):\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(session, file_name)\n",
    "    print(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Builds a chromosome from random\n",
    "The first gene determines layer types:\n",
    "    0: Inception module\n",
    "    1: Convolution layer\n",
    "    2: Fully Connected(only available for the last layer)\n",
    "    \n",
    "    Inception = [layer_type,depth_1x1,filter_depth1,dropout]\n",
    "        layer_type:   0\n",
    "        depth_1x1:    2^n, 3 <= n <= 6\n",
    "        filt_power_1: 2^m, 4 <= m <= 8\n",
    "        dropout:     True or False\n",
    "        \n",
    "    Convolution = [layer_type,filter_size,num_filters,filter_stride,pool_size,dropout]\n",
    "        layer_type:   1\n",
    "        filter_size:  2*n+1, 2 <= n <= 4\n",
    "        num_filters:  2^m,   3 <= m <= 8\n",
    "        filter_stride:j,     1 <= j <= n\n",
    "        pool_size:    k,     0 <= k <= 3, does not use pooling if k < 2\n",
    "        dropout:      True or False\n",
    "        \n",
    "    Fully Connected = [layer_type,layer_size,dropout]\n",
    "        layer_type: 2\n",
    "        layer_size: 6 <= n <= 9, 2^n\n",
    "        dropout:    True or False\n",
    "\"\"\"\n",
    "import random\n",
    "random.seed()\n",
    "# creates a chromosome with chr_size layers,\n",
    "def create_new_chromosome(chr_size=5):\n",
    "    # return list of genes\n",
    "    chromosome = []\n",
    "    \n",
    "    for i in range(chr_size):\n",
    "        # first 4 layers are only inception or conv\n",
    "        types = 1\n",
    "        \n",
    "        # last layer could be fully connected\n",
    "        if i == chr_size-1:\n",
    "            types = 2\n",
    "            \n",
    "        layer_type = random.randint(0,types)\n",
    "        \n",
    "        # inception layer\n",
    "        if layer_type == 0:\n",
    "            depth_1x1 = 2 ** (random.randint(3,6))\n",
    "            filter_depth1 = 2 ** random.randint(4,8)\n",
    "            dropout = True if random.randint(0,1) == 1 else False\n",
    "            \n",
    "            gene = [layer_type,depth_1x1,filter_depth1,dropout]\n",
    "            \n",
    "        # convolution layer\n",
    "        elif layer_type == 1:\n",
    "            filter_size_n = random.randint(2,4)\n",
    "            filter_size   = 2 * filter_size_n + 1\n",
    "            num_filters   = 2 ** random.randint(3,8)\n",
    "            filter_stride = random.randint(1,filter_size_n-1)\n",
    "            \n",
    "            pool_size = random.randint(0,3)\n",
    "            \n",
    "            dropout = True if random.randint(0,1) == 1 else False\n",
    "            \n",
    "            gene = [layer_type,filter_size,num_filters,filter_stride,pool_size,dropout]\n",
    "            \n",
    "        # fully connected layer\n",
    "        else:\n",
    "            layer_size = 2 ** random.randint(6,9)\n",
    "            dropout = True if random.randint(0,1) == 1 else False\n",
    "            gene = [layer_type,layer_size,dropout]\n",
    "            \n",
    "        chromosome.append(gene)\n",
    "        \n",
    "    return chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#builds a tensor layer from the provided gene, with the inputs\n",
    "\n",
    "def build_layer(gene, inputs):\n",
    "    input_depth = inputs.get_shape().as_list()[3]\n",
    "    \n",
    "    # inception module\n",
    "    if gene[0] == 0:\n",
    "        depth1x1 = gene[1]\n",
    "        num_filters = gene[2]\n",
    "        name = 'incep_d1x1_{}_nf{}'.format(depth1x1,num_filters)\n",
    "        layer = new_inception_layer(inputs,depth1x1,\n",
    "                                    num_filters,input_depth,name=name)\n",
    "    \n",
    "    # convolution layer\n",
    "    elif gene[0] == 1:\n",
    "        filter_size   = gene[1]\n",
    "        num_filters   = gene[2]\n",
    "        filter_stride = gene[3]\n",
    "        pool_size     = gene[4]\n",
    "        \n",
    "        name = 'conv_fsize{}_nf{}_fstr{}_pool{}'.format(filter_size,num_filters,\n",
    "                                                        filter_stride,pool_size)\n",
    "        layer, _ = new_conv_layer(input=inputs,\n",
    "                   num_input_channels=input_depth,\n",
    "                   filter_size=filter_size,\n",
    "                   num_filters=num_filters,\n",
    "                   filter_stride=filter_stride,\n",
    "                   use_pooling=pool_size,\n",
    "                   name=name)\n",
    "        \n",
    "        \n",
    "    # fully connected layer\n",
    "    # should ONLY be the last layer\n",
    "    else:\n",
    "        num_out = gene[1]\n",
    "        layer_flat, num_features = flatten_layer(inputs)\n",
    "        \n",
    "        name = 'fc_ls{}'.format(num_features)\n",
    "        layer = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=num_out,\n",
    "                         use_relu=False,\n",
    "                         name=name)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mutates a provided gene based on type\n",
    "\n",
    "def mutate(gene):\n",
    "    # if its a boolean gene, flip\n",
    "    if isinstance(gene,bool):\n",
    "        new = not gene\n",
    "\n",
    "    # its my pool_size\n",
    "    elif gene < 4:\n",
    "        new_pool = random.randint(0,3)\n",
    "        new = new_pool\n",
    "    \n",
    "    # if its an even number(power of 2)\n",
    "    elif gene%2 == 0:\n",
    "        # vary power of 2 by one in either direction, or stay the same\n",
    "        old_power = log(gene)/log(2)\n",
    "        new_power = random.randint(old_power-1,old_power+1)\n",
    "        new = 2 ** new_power\n",
    "        \n",
    "    # its an odd number\n",
    "    else:\n",
    "        # generating odd number via 2n+1, randomly generated n\n",
    "        # same drift amount as power, could be 1 below or 1 above\n",
    "        old_n = (gene-1)/2\n",
    "        new_n = random.randint(old_n-1,old_n+1)\n",
    "        new = (2 * new_n) + 1\n",
    "        \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes a list of the 3 top individuals,\n",
    "# and a mutation_rate that defaults to .5\n",
    "\n",
    "# calls mutate\n",
    "def mate(top_dogs,mutation_rate=.5):\n",
    "    new_gen = []\n",
    "    # outer loop\n",
    "    for i in range(len(top_dogs)-1):\n",
    "        # inner loop, getting so we have every individual mate\n",
    "        mom = top_dogs[i]\n",
    "        for j in range(i+1,len(top_dogs)):\n",
    "            # walking through individuals\n",
    "            dad = top_dogs[j]\n",
    "            child = []\n",
    "            \n",
    "            # walking through all layers in an individual\n",
    "            for layer in range(len(mom)):\n",
    "                new_layer = []\n",
    "                \n",
    "                # 50-50 chance we pick mom or dad's layer\n",
    "                child_layer = mom[layer] if random.random() < .5 else dad[layer]\n",
    "                \n",
    "                # not mutating layer type yet\n",
    "                new_layer.append(child_layer[0])\n",
    "                \n",
    "                # for each hyper_parameter, sans the layer_type\n",
    "                for param in range(1,len(child_layer)):\n",
    "                    new_param = child_layer[param]\n",
    "                    \n",
    "                    # mutate the hyper_param based on mutation rate\n",
    "                    if random.random() < mutation_rate:\n",
    "                        new_param = mutate(new_param)\n",
    "                    new_layer.append(new_param)\n",
    "                \n",
    "                # append the newly created layer to the child\n",
    "                child.append(new_layer)\n",
    "                \n",
    "            # we have completed child, now append to list of new generation\n",
    "            new_gen.append(child)\n",
    "    \n",
    "    # 3*(3+1)/2 --> 6\n",
    "    # elitism keeps the top 2 individuals, concate on new_gen\n",
    "    # maintains population size of 8\n",
    "    new_gen.concat(top_dogs[:2])\n",
    "    return child\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_pop(population):\n",
    "    for individual in population:\n",
    "        print(individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitness(pop):\n",
    "    fit_list = []\n",
    "    # pop[-1:-2][0] = loss\n",
    "    # pop[-1:-2][1] = time\n",
    "    \n",
    "    # average time usage over the whole population\n",
    "    ave_time = 0\n",
    "    for time in pop:\n",
    "        ave_time += time[-1:-2][1]\n",
    "    ave_time /= len(pop)\n",
    "    \n",
    "    for individual in pop:\n",
    "        #will punish times over average, and reward times under\n",
    "        time_cost = -(individual[-1:-2][1]-ave_time)*2\n",
    "        \n",
    "        #flip the loss fraction so that smaller losses are good\n",
    "        loss_fit = 1/individual[-1:-2][0]\n",
    "        \n",
    "        #fitness is the addition of these\n",
    "        #with weight on the loss\n",
    "        fitness = .7*(loss_fit) + .3*(time_cost)\n",
    "        \n",
    "        fit_list.append([fitness,individual])\n",
    "    \n",
    "    return sorted(fit_list,key=itemgetter(0),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep top 3 --> mate gives 6\n",
    "# elitism for top 2 --> gives 8\n",
    "\n",
    "# Runs genetic algorithm for num_generations of rounds, with specified pop_size\n",
    "\n",
    "def run_world(train_data,valid_data,pop_size=8,num_generations=10):\n",
    "    gen_num = 0\n",
    "    population = []\n",
    "    best = []\n",
    "\n",
    "    while gen_num <= num_generations:\n",
    "        # keep track of generation number for logging\n",
    "        gen_num += 1\n",
    "        print(\"Starting generation {}\".format(gen_num))\n",
    "        \n",
    "        # generate whole population from random if first generation\n",
    "        if gen_num == 1:\n",
    "            print(\"Generating initial population...\")\n",
    "            for i in range(pop_size):\n",
    "                population.append(create_new_chromosome(chr_size=5))\n",
    "            print_pop(population)\n",
    "\n",
    "        # otherwise, create new population via mating of top 3 individuals\n",
    "        # assuming pre-sorted based on fitness\n",
    "        else:\n",
    "            print(\"Mating top 3...\")\n",
    "            #randomly generated mutation_rate, from between 0 and .7\n",
    "            population = mate(population[:3],mutation_rate=random.random(0,.7))\n",
    "            print_pop(population)\n",
    "\n",
    "        # print(population)\n",
    "        # walk through each individual in a population\n",
    "        for i,individual in enumerate(population):\n",
    "            layer_count = {0: 0,\n",
    "                           1: 0,\n",
    "                           2: 0}\n",
    "            for gene in individual:\n",
    "                layer_count[gene[0]] += 1\n",
    "                \n",
    "            \n",
    "            # attempting to implement tensorboard for logging\n",
    "            log_string = './round_1/gen{}_ind{}_inc{}_conv{}_fc{}'.format(gen_num,i,layer_count[0],\n",
    "                                                                layer_count[1],layer_count[2])\n",
    "            \n",
    "            # takes an individual(NN architecture), sets up a tensor graph according to genes\n",
    "            # returns the final Validation set-loss, and batch_time for evaluating fitness\n",
    "            loss,time = evaluate(individual,i,\n",
    "                                 train_data,valid_data,\n",
    "                                 log_string)\n",
    "            print(\"\\tBatch-time: time = {}\".format(time))\n",
    "\n",
    "            # concat loss and time of model on end of individual\n",
    "            individual += [loss,time]\n",
    "\n",
    "        # evaluate fitness of population and return sorted, with best at top\n",
    "        fit_list = fitness(population)\n",
    "\n",
    "        # copy sorted version of population\n",
    "        # save the best individual of this generation to its own list for review\n",
    "        best.append(fit_list[0])\n",
    "        print(\"best for gen{} is: \".format(generation))\n",
    "        print(fit_list[0])\n",
    "        \n",
    "        # copy sorted version of population\n",
    "        # last item element is loss and time, unneeded for new population\n",
    "        population = [individual[:len(individual)-1] for individual in fit_list]\n",
    "        \n",
    "    legend = sorted(fit_list,key=itemgetter(0),reverse=True)\n",
    "    print(\"The best of the best is: \")\n",
    "    print(legend[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Build and Run Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sets up tensor graph and runs for specified num of epochs,\n",
    "\n",
    "def evaluate(individual,indiv_num,\n",
    "             train_data,valid_data,\n",
    "             log_string,epochs=1):\n",
    "    \n",
    "    train_features, train_labels = train_data\n",
    "    valid_features, valid_labels = valid_data\n",
    "    train_batch_size = 256\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    learning_rate=1e-4\n",
    "    file_writer = tf.summary.FileWriter(log_string)\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, img_height, img_width, num_channels], name='x')\n",
    "   \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    #create actual tensor graph from the genes in an individual\n",
    "    model = create_model(individual,x,keep_prob)\n",
    "    with tf.name_scope('targets'):\n",
    "        y_true = tf.placeholder(tf.float32, shape=[None,num_classes], name = 'y_true')\n",
    "        y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "        \n",
    "    # apply softmax to turn class prediction into probabilities\n",
    "    with tf.name_scope('predictions'):\n",
    "        y_pred = tf.nn.softmax(model)\n",
    "        y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        mcll_cost = tf.losses.log_loss(y_true,y_pred,epsilon=1e-15)\n",
    "        tf.summary.scalar('loss',mcll_cost)\n",
    "        \n",
    "    with tf.name_scope('train'):\n",
    "        mcll_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mcll_cost)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    #tensors that need to passed to other functions\n",
    "    test_stuff = (x,y_true,keep_prob,y_pred_cls,mcll_cost)\n",
    "    \n",
    "    #merging all of tensorboard elements\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    #initialize the batch_time that will be returned\n",
    "    batch_time = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            #re-shuffle data on each round\n",
    "            train_features, train_labels = shuffle(train_features, train_labels)\n",
    "\n",
    "            #seperate into batches\n",
    "            batch_data, y_true_batch = fetch_batches(train_batch_size, train_features, train_labels)\n",
    "            \n",
    "            #run inside loop over-batches, iterating i inside\n",
    "            loss = 0\n",
    "            i=0\n",
    "            for x_batch, y_true_batch in zip(batch_data, y_true_batch):\n",
    "\n",
    "                #tensor_feed dictionary for place_holders defined above\n",
    "                feed_dict = {x: x_batch,\n",
    "                                   y_true: y_true_batch,\n",
    "                                   keep_prob: .5}\n",
    "                \n",
    "                if e == 0:\n",
    "                    start_time = time.time()\n",
    "                summary, batch_loss, _ = sess.run([merged,mcll_cost,\n",
    "                                                   mcll_optimizer],\n",
    "                                                   feed_dict=feed_dict)\n",
    "                if e==0:\n",
    "                    end_time = time.time()\n",
    "                    batch_time += end_time - start_time\n",
    "                \n",
    "                loss += batch_loss\n",
    "                file_writer.add_summary(summary)\n",
    "                if i==0 or i==9:\n",
    "                    print(\"num {}: epoch {}: batch{}\".format(indiv_num+1,e+1,i+1))\n",
    "                i+=1\n",
    "                \n",
    "            print('Train Loss for Epoch {}: {}'.format(e+1,loss))\n",
    "            \n",
    "            #calculate the val_loss and accuracy on validation set after each epoch\n",
    "            val_loss = print_valid(sess,test_stuff,valid_features, valid_labels,256)\n",
    "                \n",
    "        #return val_loss and batch_time for fitness evaluation\n",
    "        return val_loss, batch_time\n",
    "    \n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#runs tensor session over validation set, returning class predictions for accuracy, and cost for total loss\n",
    "\n",
    "def print_valid(sess,test_stuff, features, labels, batch_size):\n",
    "    #number of test examples\n",
    "    num_test = len(features)\n",
    "    x,y_true,keep_prob,y_pred_cls,mcll_cost = test_stuff\n",
    "    \n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "    correct_sum = 0\n",
    "    i=0\n",
    "    cost=0\n",
    "    while i < num_test:\n",
    "        \n",
    "        #ending point of batch, either batch size or the end of the test set\n",
    "        j = min(i+batch_size, num_test)\n",
    "        \n",
    "        #batch o' test material\n",
    "        test_batch  = features[i:j]\n",
    "        test_lab = labels[i:j]\n",
    "        \n",
    "        #feed dat tensor sesh\n",
    "        feed_dict = {x: test_batch,\n",
    "                     y_true: test_lab,\n",
    "                     keep_prob: 1.0}\n",
    "        \n",
    "        #the predictions for this batch\n",
    "        cls_pred[i:j],batch_cost = sess.run([y_pred_cls,mcll_cost], feed_dict=feed_dict)\n",
    "        \n",
    "        #move the starting point\n",
    "        i=j\n",
    "        cost += batch_cost\n",
    "    for p,l in zip(cls_pred, labels):\n",
    "        if l[p] == 1:\n",
    "            correct_sum += 1\n",
    "            \n",
    "    acc = float(correct_sum) / num_test\n",
    "    \n",
    "    acc_msg = \"\\tAccuracy on Valid set: {0:.1%} ({1} / {2})\"\n",
    "    loss_msg = \"\\tLoss on Valid-Set: {0}\"\n",
    "    print(acc_msg.format(acc, correct_sum, num_test))\n",
    "    print(loss_msg.format(cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates an actual model from the provided chromosome\n",
    "# calls build_layer\n",
    "\n",
    "def create_model(chromosome,x,keep_prob):\n",
    "    # create initial model from input data\n",
    "    model = build_layer(chromosome[0],x)\n",
    "    if chromosome[0][-1]:\n",
    "        model = tf.nn.dropout(model,keep_prob)\n",
    "    \n",
    "    # skip the first\n",
    "    for i in range(1,len(chromosome)):\n",
    "        gene = chromosome[i]\n",
    "        model = build_layer(gene,model)\n",
    "        \n",
    "        # The last item in each list is bool for whether or not dropout is used\n",
    "        if gene[-1]:\n",
    "            model = tf.nn.dropout(model,keep_prob)\n",
    "            \n",
    "    # if the last layer was not fully connect,\n",
    "    # needs to be flattened before sending to fully_connectged output layer\n",
    "    if gene[0] != 2:\n",
    "        # if the layer was not flattened, the input for my FC output\n",
    "        # is num_features returned by my flattening function\n",
    "        model, num_features = flatten_layer(model)\n",
    "        \n",
    "    else:\n",
    "        # otherwise, it is the size of my previous fully connected layer\n",
    "        num_features = gene[1]\n",
    "        \n",
    "    output_layer = new_fc_layer(input=model,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False,name='output')\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = 'train.zip'\n",
    "train_data,valid_data = data_prep(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convolution layer, non-matching layers\n",
    "#inception module, wrong args\n",
    "\n",
    "run_world(train_data,valid_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
